# -*- coding: utf-8 -*-
"""Indic_GPT2_AI4B_010

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14j5zrWfIF9KhkISaYy53XWTkRweDybhk
"""

# Load Google Drive

from google.colab import drive
drive.mount('/content/drive')

"""**Tokenize the Dataset through Indic NLP Tokenizer**"""

1.1

# Install Indic NLP Library

!pip install indic-nlp-library

1.2

# Download Indic NLP Resources

!wget https://github.com/anoopkunchukuttan/indic_nlp_resources/archive/refs/heads/master.zip -O /content/drive/MyDrive/Indic_GPT2/indic_nlp_resources.zip

1.3

# Extract files

import zipfile

zip_path = "/content/drive/MyDrive/Indic_GPT2/indic_nlp_resources.zip"
extract_path = "/content/drive/MyDrive/Indic_GPT2/indic_nlp_resources"

# Extract the zip file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Extraction complete! Resources are saved in:", extract_path)

1.4

# Set up Resource Path

from indicnlp import common

# Set the path to the extracted resources
INDIC_NLP_RESOURCES = "/content/drive/MyDrive/Indic_GPT2/indic_nlp_resources"
common.set_resources_path(INDIC_NLP_RESOURCES)

print("Resource path set successfully!")

1.5

# Define a Tokenization Function

from indicnlp.tokenize import indic_tokenize

# Define a function to tokenize text
def tokenize_text(example):
    # Replace 'hi' with the language code for your dataset (e.g., 'ta' for Tamil, 'bn' for Bengali)
    tokenized_text = indic_tokenize.trivial_tokenize(example["text"], lang="hi")
    example["tokenized_text"] = " ".join(tokenized_text)  # Join tokens with spaces
    return example

print("Tokenization function is ready!")

1.6

# Install the datasets library
!pip install datasets

1.7

from datasets import load_from_disk

# Load the dataset using load_from_disk
dataset = load_from_disk("/content/drive/MyDrive/Indic_GPT2/AI4Bharat_Dataset/Verified_Hindi_p1_combined")

# Apply IndicNLPtokenization to the dataset
tokenized_dataset = dataset.map(tokenize_text, num_proc=4)  # Adjust num_proc based on resources

print("Tokenization completed!")

# Save the IndicNLP tokenized dataset to Google Drive
save_path = "/content/drive/MyDrive/Indic_GPT2/AI4B010_IndicNLP_tokenized_dataset"
tokenized_dataset.save_to_disk(save_path)

print(f"Tokenized dataset saved successfully at {save_path}")

"""**Map the IndicNLP Tokenized Dataset to GPT 2 Compatible Tokenizer**"""

2.1

# Install Hugging Face Transformers

!pip install transformers

2.2

# Load the GPT-2 Tokenizer with Padding

from transformers import GPT2Tokenizer

# Load GPT-2 tokenizer
gpt2_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Set the padding token to be the same as the EOS token
gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token

# Verify the padding token
print("Padding token set to:", gpt2_tokenizer.pad_token)

2.3

# Map the IndicNLP Tokenized Dataset to GPT-2 Tokenizer with Padding

def map_to_gpt2_tokenizer(example):
    # Tokenize with GPT-2 tokenizer, applying truncation and padding
    gpt2_tokenized = gpt2_tokenizer(
        example["tokenized_text"],
        truncation=True,
        padding="max_length",  # Pad to max_length
        max_length=128,        # Adjust based on model/dataset needs
        return_tensors="np"    # Return NumPy arrays for compatibility
    )
    # Return the input IDs and attention masks
    return {
        "input_ids": gpt2_tokenized["input_ids"].tolist(),
        "attention_mask": gpt2_tokenized["attention_mask"].tolist()
    }

# Apply mapping to the entire dataset
gpt2_mapped_dataset = tokenized_dataset.map(map_to_gpt2_tokenizer, batched=True)

print("Mapping to GPT-2 tokenizer with padding completed!")

# Save the mapped dataset to the specified folder
save_path = "/content/drive/MyDrive/Indic_GPT2/gpt2_ai4b010_tokenized"
gpt2_mapped_dataset.save_to_disk(save_path)

print(f"Tokenized dataset saved to: {save_path}")

2.4

# Save the GPT-2 Compatible Dataset

gpt2_save_path = "/content/drive/MyDrive/Indic_GPT2/gpt2_mapped_dataset_ai4b010"
gpt2_mapped_dataset.save_to_disk(gpt2_save_path)

print(f"GPT-2 compatible dataset saved successfully at {gpt2_save_path}")